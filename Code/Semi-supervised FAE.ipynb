{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da7637a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Regular Functional Autoencoder (FAE) - Clean/Basics Version\n",
    "# - Unsupervised FAE training (reconstruction + optional penalty)\n",
    "# - Logistic regression on encoder representations\n",
    "# - History for plots every log_every epochs\n",
    "# - Optional \"extras\" saved for latent/confusion/reconstruction plots\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from skfda import representation\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 0) Reproducibility\n",
    "# -------------------------\n",
    "def set_seed(seed):\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 1) Basis construction\n",
    "# -------------------------\n",
    "def build_basis_fc(tpts, n_basis=20, basis_type=\"Bspline\", custom_basis_fn=None):\n",
    "    \"\"\"\n",
    "    Returns basis matrix evaluated on tpts.\n",
    "\n",
    "    Output shape: [n_time, n_basis]\n",
    "\n",
    "    custom_basis_fn:\n",
    "        function custom_basis_fn(tpts) -> array/tensor shape [n_time, n_basis]\n",
    "        If provided, this overrides basis_type.\n",
    "    \"\"\"\n",
    "    # Custom basis\n",
    "    if custom_basis_fn is not None:\n",
    "        B = custom_basis_fn(tpts)\n",
    "        if not torch.is_tensor(B):\n",
    "            B = torch.tensor(B, dtype=torch.float32)\n",
    "        return B.float()\n",
    "\n",
    "    # Built-in basis from scikit-fda\n",
    "    t = tpts.flatten().detach().cpu().numpy()\n",
    "    t_min, t_max = float(t.min()), float(t.max())\n",
    "\n",
    "    if basis_type == \"Bspline\":\n",
    "        # newer API: BSplineBasis\n",
    "        basis = representation.basis.BSplineBasis(n_basis=n_basis, order=4)\n",
    "        eval_ = basis(tpts.detach().cpu().numpy(), derivative=0)[:, :, 0]  # [n_time, n_basis]\n",
    "    elif basis_type == \"Fourier\":\n",
    "        basis = representation.basis.Fourier([t_min, t_max], n_basis=n_basis)\n",
    "        eval_ = basis(tpts.detach().cpu().numpy(), derivative=0)[:, :, 0]  # [n_time, n_basis]\n",
    "    else:\n",
    "        raise ValueError(\"basis_type must be 'Bspline' or 'Fourier'\")\n",
    "\n",
    "    return torch.from_numpy(eval_).float()\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 2) FAE model (linear)\n",
    "# -------------------------\n",
    "class FAEVanilla(nn.Module):\n",
    "    def __init__(self, n_basis_project, n_rep, n_basis_revert, init_weight_sd=None):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Linear(n_basis_project, n_rep, bias=False)\n",
    "        self.decoder = nn.Linear(n_rep, n_basis_revert, bias=False)\n",
    "        self.activation = nn.Identity()  # linear FAE\n",
    "\n",
    "        if init_weight_sd is not None:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, nn.Linear):\n",
    "                    nn.init.normal_(m.weight, mean=0.0, std=init_weight_sd)\n",
    "\n",
    "    def project(self, x, tpts, basis_fc):\n",
    "        \"\"\"\n",
    "        x: [batch, n_time]\n",
    "        basis_fc: [n_time, n_basis] OR [n_basis, n_time] (we handle both)\n",
    "        returns: [batch, n_basis]\n",
    "        \"\"\"\n",
    "        t = tpts.flatten()\n",
    "        dt = t[1:] - t[:-1]\n",
    "\n",
    "        zero = torch.zeros(1, device=x.device, dtype=x.dtype)\n",
    "        W = 0.5 * torch.cat([zero, dt]) + 0.5 * torch.cat([dt, zero])  # [n_time]\n",
    "\n",
    "        n_time = x.shape[1]\n",
    "        # Make B be [n_time, n_basis]\n",
    "        if basis_fc.shape[0] == n_time:\n",
    "            B = basis_fc\n",
    "        elif basis_fc.shape[1] == n_time:\n",
    "            B = basis_fc.T\n",
    "        else:\n",
    "            raise RuntimeError(f\"basis_fc shape {tuple(basis_fc.shape)} not compatible with n_time={n_time}\")\n",
    "\n",
    "        return (x * W) @ B  # [batch, n_basis]\n",
    "\n",
    "    def revert(self, coef, basis_fc):\n",
    "        \"\"\"\n",
    "        coef: [batch, n_basis]\n",
    "        basis_fc: [n_time, n_basis] OR [n_basis, n_time]\n",
    "        returns: [batch, n_time]\n",
    "        \"\"\"\n",
    "        n_basis = coef.shape[1]\n",
    "        # want output [batch, n_time]\n",
    "        if basis_fc.shape[1] == n_basis:\n",
    "            # basis_fc is [n_time, n_basis] -> coef @ basis_fc.T\n",
    "            return coef @ basis_fc.T\n",
    "        elif basis_fc.shape[0] == n_basis:\n",
    "            # basis_fc is [n_basis, n_time] -> coef @ basis_fc\n",
    "            return coef @ basis_fc\n",
    "        else:\n",
    "            raise RuntimeError(f\"basis_fc shape {tuple(basis_fc.shape)} not compatible with n_basis={n_basis}\")\n",
    "\n",
    "    def forward(self, x, tpts, basis_fc_project, basis_fc_revert):\n",
    "        feature = self.project(x, tpts, basis_fc_project)   # [batch, n_basis_project]\n",
    "        rep = self.activation(self.encoder(feature))        # [batch, n_rep]\n",
    "        coef = self.decoder(rep)                            # [batch, n_basis_revert]\n",
    "        x_hat = self.revert(coef, basis_fc_revert)          # [batch, n_time]\n",
    "        return x_hat, rep, feature, coef\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 3) Penalty (same as your original)\n",
    "# -------------------------\n",
    "def diff_penalty(coef):\n",
    "    # coef: [batch, n_basis]\n",
    "    delta = coef[:, 2:] - 2 * coef[:, 1:-1] + coef[:, :-2]\n",
    "    return torch.mean(torch.sum(delta**2, dim=1))\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 4) Train / evaluate\n",
    "# -------------------------\n",
    "def train_one_epoch(model, loader, optimizer, loss_fn, device,\n",
    "                    tpts, basis_fc_project, basis_fc_revert,\n",
    "                    pen=None, lamb=0.0):\n",
    "\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_score_loss = 0.0\n",
    "    n_batches = 0\n",
    "\n",
    "    for x_batch in loader:\n",
    "        x_batch = x_batch.to(device).float()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        x_hat, rep, feature, coef = model(x_batch, tpts, basis_fc_project, basis_fc_revert)\n",
    "\n",
    "        # recon loss\n",
    "        loss = loss_fn(x_hat, x_batch)\n",
    "\n",
    "        # score loss (optional; only meaningful in special cases)\n",
    "        score_loss = 0.0\n",
    "        if feature.shape == coef.shape:\n",
    "            score_loss = loss_fn(feature, coef)\n",
    "            total_score_loss += float(score_loss.detach().cpu())\n",
    "\n",
    "        # penalty\n",
    "        if pen == \"diff\" and lamb > 0:\n",
    "            loss = loss + lamb * diff_penalty(coef)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += float(loss.detach().cpu())\n",
    "        n_batches += 1\n",
    "\n",
    "    return total_loss / max(n_batches, 1), total_score_loss / max(n_batches, 1)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, x_data, loss_fn, device, tpts, basis_fc_project, basis_fc_revert):\n",
    "    model.eval()\n",
    "    x_data = x_data.to(device).float()\n",
    "    x_hat, rep, feature, coef = model(x_data, tpts, basis_fc_project, basis_fc_revert)\n",
    "    loss = loss_fn(x_hat, x_data)\n",
    "    return x_hat, rep, float(loss.detach().cpu()), coef\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 5) Main experiment runner\n",
    "# -------------------------\n",
    "def run_fae_experiment(\n",
    "    x, label, tpts,\n",
    "    n_iter=5,\n",
    "    split_rate=0.8,\n",
    "    epochs=2000,\n",
    "    batch_size=28,\n",
    "    n_rep=5,\n",
    "    n_basis_project=20,\n",
    "    n_basis_revert=20,\n",
    "    basis_type_project=\"Bspline\",\n",
    "    basis_type_revert=\"Bspline\",\n",
    "    custom_basis_project_fn=None,\n",
    "    custom_basis_revert_fn=None,\n",
    "    pen=\"diff\",\n",
    "    lamb=0.001,\n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-6,\n",
    "    init_weight_sd=0.5,\n",
    "    log_every=100,\n",
    "    device=\"cpu\",\n",
    "    seed=743,\n",
    "    save_extras=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      history dict with:\n",
    "        - train/test final recon loss per replicate\n",
    "        - train/test final classification accuracy per replicate\n",
    "        - per-100-epoch test recon loss + test acc (lists across replicates)\n",
    "        - optional extras for plotting (last replicate)\n",
    "    \"\"\"\n",
    "    set_seed(seed)\n",
    "    device = torch.device(device)\n",
    "\n",
    "    # basis matrices\n",
    "    basis_fc_project = build_basis_fc(\n",
    "        tpts, n_basis=n_basis_project, basis_type=basis_type_project, custom_basis_fn=custom_basis_project_fn\n",
    "    ).to(device)\n",
    "\n",
    "    basis_fc_revert = build_basis_fc(\n",
    "        tpts, n_basis=n_basis_revert, basis_type=basis_type_revert, custom_basis_fn=custom_basis_revert_fn\n",
    "    ).to(device)\n",
    "\n",
    "    tpts = tpts.to(device).float()\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    history = {\n",
    "        \"train_recon_loss\": [],\n",
    "        \"test_recon_loss\": [],\n",
    "        \"train_class_acc\": [],\n",
    "        \"test_class_acc\": [],\n",
    "        \"per_epoch_test_recon_loss\": [[] for _ in range(epochs // log_every)],\n",
    "        \"per_epoch_test_class_acc\": [[] for _ in range(epochs // log_every)],\n",
    "    }\n",
    "\n",
    "    # make sure label is numpy\n",
    "    y = np.asarray(label)\n",
    "    idx_all = np.arange(x.shape[0])\n",
    "\n",
    "    # replicate loop\n",
    "    for r in range(n_iter):\n",
    "        # stratified split helps confusion matrix a lot\n",
    "        idx_train, idx_test = train_test_split(\n",
    "            idx_all,\n",
    "            train_size=split_rate,\n",
    "            random_state=seed + r,\n",
    "            shuffle=True,\n",
    "            stratify=y\n",
    "        )\n",
    "\n",
    "        TrainData = x[idx_train]\n",
    "        TestData  = x[idx_test]\n",
    "        TrainLabel = y[idx_train]\n",
    "        TestLabel  = y[idx_test]\n",
    "\n",
    "        train_loader = DataLoader(TrainData, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        # local model (fresh each replicate)\n",
    "        model = FAEVanilla(n_basis_project, n_rep, n_basis_revert, init_weight_sd=init_weight_sd).to(device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "        # training loop\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            train_loss, score_loss = train_one_epoch(\n",
    "                model, train_loader, optimizer, loss_fn, device,\n",
    "                tpts, basis_fc_project, basis_fc_revert,\n",
    "                pen=pen, lamb=lamb\n",
    "            )\n",
    "\n",
    "            if epoch % log_every == 0:\n",
    "                # reps + test loss\n",
    "                _, reps_test, test_loss, _ = evaluate(model, TestData, loss_fn, device, tpts, basis_fc_project, basis_fc_revert)\n",
    "                _, reps_train, _, _        = evaluate(model, TrainData, loss_fn, device, tpts, basis_fc_project, basis_fc_revert)\n",
    "\n",
    "                # modern LR (no warnings)\n",
    "                clf = LogisticRegression(solver=\"lbfgs\", max_iter=2000, random_state=0)\n",
    "                clf.fit(reps_train.detach().cpu().numpy(), TrainLabel)\n",
    "                test_acc = clf.score(reps_test.detach().cpu().numpy(), TestLabel)\n",
    "\n",
    "                k = (epoch // log_every) - 1\n",
    "                history[\"per_epoch_test_recon_loss\"][k].append(test_loss)\n",
    "                history[\"per_epoch_test_class_acc\"][k].append(float(test_acc))\n",
    "\n",
    "                print(f\"[rep {r+1}/{n_iter}] epoch {epoch:4d} | train={train_loss:.4f} | test={test_loss:.4f} | test_acc={test_acc:.4f}\")\n",
    "\n",
    "        # final eval\n",
    "        _, reps_test, test_loss, _  = evaluate(model, TestData,  loss_fn, device, tpts, basis_fc_project, basis_fc_revert)\n",
    "        _, reps_train, train_loss, _ = evaluate(model, TrainData, loss_fn, device, tpts, basis_fc_project, basis_fc_revert)\n",
    "\n",
    "        clf = LogisticRegression(solver=\"lbfgs\", max_iter=2000, random_state=0)\n",
    "        clf.fit(reps_train.detach().cpu().numpy(), TrainLabel)\n",
    "\n",
    "        history[\"train_recon_loss\"].append(train_loss)\n",
    "        history[\"test_recon_loss\"].append(test_loss)\n",
    "        history[\"train_class_acc\"].append(float(clf.score(reps_train.detach().cpu().numpy(), TrainLabel)))\n",
    "        history[\"test_class_acc\"].append(float(clf.score(reps_test.detach().cpu().numpy(), TestLabel)))\n",
    "\n",
    "        print(f\"Replicate {r+1} complete.\\n\")\n",
    "\n",
    "        # save extras (last replicate) so you can plot latent/confusion/recon easily\n",
    "        if save_extras and (r == n_iter - 1):\n",
    "            history[\"extras\"] = {\n",
    "                \"model\": model,\n",
    "                \"TrainData\": TrainData,\n",
    "                \"TestData\": TestData,\n",
    "                \"TrainLabel\": TrainLabel,\n",
    "                \"TestLabel\": TestLabel,\n",
    "                \"tpts\": tpts,\n",
    "                \"basis_fc_project\": basis_fc_project,\n",
    "                \"basis_fc_revert\": basis_fc_revert,\n",
    "                \"device\": device,\n",
    "                \"loss_fn\": loss_fn,\n",
    "            }\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0b617b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "test_mse_mean = [np.mean(v) for v in history[\"per_epoch_test_recon_loss\"]]\n",
    "test_mse_sd   = [np.std(v)  for v in history[\"per_epoch_test_recon_loss\"]]\n",
    "\n",
    "test_acc_mean = [np.mean(v) for v in history[\"per_epoch_test_class_acc\"]]\n",
    "test_acc_sd   = [np.std(v)  for v in history[\"per_epoch_test_class_acc\"]]\n",
    "\n",
    "epochs_grid = np.arange(100, len(test_mse_mean)*100 + 1, 100)\n",
    "\n",
    "plt.figure()\n",
    "plt.errorbar(epochs_grid, test_mse_mean, yerr=test_mse_sd, fmt='-o', capsize=3)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Test MSE')\n",
    "plt.title('FAE reconstruction error over training')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.errorbar(epochs_grid, test_acc_mean, yerr=test_acc_sd, fmt='-o', capsize=3)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Test classification accuracy')\n",
    "plt.ylim(0, 1)\n",
    "plt.title('FAE classification accuracy over training')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7202eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "extras = history[\"extras\"]\n",
    "model = extras[\"model\"]\n",
    "loss_fn = extras[\"loss_fn\"]\n",
    "device = extras[\"device\"]\n",
    "tpts = extras[\"tpts\"]\n",
    "basis_fc_project = extras[\"basis_fc_project\"]\n",
    "basis_fc_revert = extras[\"basis_fc_revert\"]\n",
    "\n",
    "FAE_pred_all, FAE_reps_all, _, _ = evaluate(model, x, loss_fn, device, tpts, basis_fc_project, basis_fc_revert)\n",
    "\n",
    "z = FAE_reps_all.detach().cpu().numpy()\n",
    "lbl = np.asarray(label)\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "sc = plt.scatter(z[:, 0], z[:, 1], c=lbl, cmap='viridis')\n",
    "plt.xlabel('FAE representation dim 1')\n",
    "plt.ylabel('FAE representation dim 2')\n",
    "plt.title('FAE latent space colored by labels (last replicate)')\n",
    "plt.colorbar(sc, label='Label')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57487d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "extras = history[\"extras\"]\n",
    "TrainData = extras[\"TrainData\"]\n",
    "TestData = extras[\"TestData\"]\n",
    "TrainLabel = extras[\"TrainLabel\"]\n",
    "TestLabel = extras[\"TestLabel\"]\n",
    "model = extras[\"model\"]\n",
    "loss_fn = extras[\"loss_fn\"]\n",
    "device = extras[\"device\"]\n",
    "tpts = extras[\"tpts\"]\n",
    "basis_fc_project = extras[\"basis_fc_project\"]\n",
    "basis_fc_revert = extras[\"basis_fc_revert\"]\n",
    "\n",
    "_, reps_train, _, _ = evaluate(model, TrainData, loss_fn, device, tpts, basis_fc_project, basis_fc_revert)\n",
    "_, reps_test,  _, _ = evaluate(model, TestData,  loss_fn, device, tpts, basis_fc_project, basis_fc_revert)\n",
    "\n",
    "clf = LogisticRegression(solver=\"lbfgs\", max_iter=2000, random_state=0)\n",
    "clf.fit(reps_train.detach().cpu().numpy(), TrainLabel)\n",
    "\n",
    "y_test_pred = clf.predict(reps_test.detach().cpu().numpy())\n",
    "\n",
    "classes = np.unique(np.concatenate([TestLabel, y_test_pred]))\n",
    "cm = confusion_matrix(TestLabel, y_test_pred, labels=classes)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 4))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)\n",
    "disp.plot(ax=ax)\n",
    "ax.set_title('FAE + Logistic Regression\\nConfusion matrix (last replicate)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87db305",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "extras = history[\"extras\"]\n",
    "model = extras[\"model\"]\n",
    "loss_fn = extras[\"loss_fn\"]\n",
    "device = extras[\"device\"]\n",
    "tpts = extras[\"tpts\"]\n",
    "basis_fc_project = extras[\"basis_fc_project\"]\n",
    "basis_fc_revert = extras[\"basis_fc_revert\"]\n",
    "\n",
    "FAE_pred_all, _, _, _ = evaluate(model, x, loss_fn, device, tpts, basis_fc_project, basis_fc_revert)\n",
    "\n",
    "t = tpts.detach().cpu().numpy().flatten()\n",
    "idx_list = [10, 50, 120, 250]\n",
    "\n",
    "for idx in idx_list:\n",
    "    orig = x[idx].detach().cpu().numpy()\n",
    "    recon = FAE_pred_all[idx].detach().cpu().numpy()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(t, orig, label='Original', linewidth=2)\n",
    "    plt.plot(t, recon, '--', label='FAE reconstruction', linewidth=2)\n",
    "    plt.title(f'Curve {idx}')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-iris",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
