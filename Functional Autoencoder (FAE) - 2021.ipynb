{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd69d66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functional Autoencoder in PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class FourierBasis(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple Fourier basis on [0, 1] for representing functional weights.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_basis: int, T: int, device=None, dtype=torch.float32):\n",
    "        super().__init__()\n",
    "        self.num_basis = num_basis\n",
    "        self.T = T\n",
    "        device = device or torch.device(\"cpu\")\n",
    "\n",
    "        # time grid in [0, 1]\n",
    "        t = torch.linspace(0.0, 1.0, T, dtype=dtype, device=device)  # [T]\n",
    "        self.register_buffer(\"t\", t)\n",
    "\n",
    "        # Build basis matrix Phi[b, t] = phi_b(t)\n",
    "        # basis 0: constant; others: sines and cosines\n",
    "        Phi = torch.zeros(num_basis, T, dtype=dtype, device=device)\n",
    "        Phi[0] = 1.0\n",
    "        k = 1\n",
    "        freq = 1\n",
    "        while k < num_basis:\n",
    "            Phi[k] = torch.sin(2 * torch.pi * freq * t)\n",
    "            k += 1\n",
    "            if k < num_basis:\n",
    "                Phi[k] = torch.cos(2 * torch.pi * freq * t)\n",
    "                k += 1\n",
    "            freq += 1\n",
    "\n",
    "        self.register_buffer(\"Phi\", Phi)\n",
    "\n",
    "        # approximate dt for Riemann sum\n",
    "        dt = (t[-1] - t[0]) / (T - 1)\n",
    "        self.register_buffer(\"dt\", torch.tensor(dt, dtype=dtype, device=device))\n",
    "\n",
    "\n",
    "class FunctionalAutoencoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Functional Autoencoder (FAE) for P-dimensional functional data.\n",
    "\n",
    "    x: [batch, P, T]  (P channels, T time points)\n",
    "    latent_dim: dimension d of encoding.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        P: int,\n",
    "        T: int,\n",
    "        latent_dim: int,\n",
    "        num_basis: int = 16,\n",
    "        hidden_dims=None,\n",
    "        device=None,\n",
    "        dtype=torch.float32,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        device = device or torch.device(\"cpu\")\n",
    "        self.P = P\n",
    "        self.T = T\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        # Basis to represent functional weights\n",
    "        self.basis = FourierBasis(num_basis=num_basis, T=T, device=device, dtype=dtype)\n",
    "\n",
    "        B = num_basis\n",
    "\n",
    "        ### Functional first layer: HP -> R^H1 ### \n",
    "        # coefficients for w^{(1)}_{k,j}(t): shape [H1, P, B]\n",
    "        H1 = hidden_dims[0] if hidden_dims is not None else latent_dim\n",
    "        self.w1_coeffs = nn.Parameter(\n",
    "            torch.randn(H1, P, B, dtype=dtype, device=device) * 0.1\n",
    "        )\n",
    "\n",
    "        # optional vector-to-vector hidden layers (between functional first layer and latent)\n",
    "        fc_layers = []\n",
    "        in_dim = H1\n",
    "        if hidden_dims is not None and len(hidden_dims) > 1:\n",
    "            for h in hidden_dims[1:]:\n",
    "                fc_layers.append(nn.Linear(in_dim, h, bias=True))\n",
    "                fc_layers.append(nn.Tanh())\n",
    "                in_dim = h\n",
    "        # final linear to latent representation\n",
    "        fc_layers.append(nn.Linear(in_dim, latent_dim, bias=True))\n",
    "        self.encoder_mlp = nn.Sequential(*fc_layers)\n",
    "\n",
    "        # --- Functional last layer: R^latent -> HP ---\n",
    "        # coefficients for w^{(last)}_{j,k}(t):\n",
    "        # shape [P, latent_dim, B]\n",
    "        self.w_last_coeffs = nn.Parameter(\n",
    "            torch.randn(P, latent_dim, B, dtype=dtype, device=device) * 0.1\n",
    "        )\n",
    "\n",
    "        self.activation = torch.tanh  # can be ReLU / etc.\n",
    "\n",
    "    # ----- helper: build functional weights on the time grid -----\n",
    "    def _weights_from_coeffs(self, coeffs):\n",
    "        \"\"\"\n",
    "        coeffs: [..., B]\n",
    "        basis.Phi: [B, T]\n",
    "        returns: [..., T]\n",
    "        \"\"\"\n",
    "        # Einstein summation: (..., B) * (B, T) -> (..., T)\n",
    "        return torch.einsum(\"...b,bt->...t\", coeffs, self.basis.Phi)\n",
    "\n",
    "    # ----- encoder functional layer -----\n",
    "    def _encode_first_layer(self, x):\n",
    "        \"\"\"\n",
    "        x: [B, P, T]\n",
    "        returns hidden_1: [B, H1]\n",
    "        \"\"\"\n",
    "        Bbatch = x.shape[0]\n",
    "        dt = self.basis.dt\n",
    "\n",
    "        # w1_funcs: [H1, P, T]\n",
    "        w1_funcs = self._weights_from_coeffs(self.w1_coeffs)\n",
    "\n",
    "        # inner product <x_j, w_{k,j}>_L2  ≈ Σ_t x_j(t) w_{k,j}(t) dt\n",
    "        # x -> [B, 1, P, T]\n",
    "        # w1_funcs -> [1, H1, P, T]\n",
    "        prod = x.unsqueeze(1) * w1_funcs.unsqueeze(0)  # [B, H1, P, T]\n",
    "        inner_per_channel = prod.sum(dim=-1) * dt      # [B, H1, P]\n",
    "\n",
    "        # sum over P dimensions\n",
    "        h_raw = inner_per_channel.sum(dim=-1)  # [B, H1]\n",
    "\n",
    "        h = self.activation(h_raw)\n",
    "        return h\n",
    "\n",
    "    # ----- decoder functional layer -----\n",
    "    def _decode_last_layer(self, z):\n",
    "        \"\"\"\n",
    "        z: [B, latent_dim]\n",
    "        returns x_hat: [B, P, T]\n",
    "        \"\"\"\n",
    "        # w_last_funcs: [P, latent_dim, T]\n",
    "        w_last_funcs = self._weights_from_coeffs(self.w_last_coeffs)\n",
    "\n",
    "        # x_hat_j(t) = Σ_k z_k * w_{j,k}(t)\n",
    "        # z: [B, K], w_last_funcs: [P, K, T] -> [B, P, T]\n",
    "        x_hat = torch.einsum(\"bk,pkt->bpt\", z, w_last_funcs)\n",
    "        return x_hat\n",
    "\n",
    "    # ----- full forward pass -----\n",
    "    def encode(self, x):\n",
    "        \"\"\"\n",
    "        x: [batch, P, T]\n",
    "        returns z: [batch, latent_dim]\n",
    "        \"\"\"\n",
    "        h1 = self._encode_first_layer(x)\n",
    "        z = self.encoder_mlp(h1)\n",
    "        return z\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self._decode_last_layer(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encode(x)\n",
    "        x_hat = self.decode(z)\n",
    "        return x_hat, z\n",
    "\n",
    "\n",
    "def functional_mse_loss(x, x_hat, dt):\n",
    "    \"\"\"\n",
    "    Approximate functional L2 reconstruction loss:\n",
    "    (1 / 2n) sum_i sum_j ∫ (x - x_hat)^2 dt\n",
    "    x, x_hat: [B, P, T]\n",
    "    \"\"\"\n",
    "    diff2 = (x - x_hat) ** 2  # [B, P, T]\n",
    "    # integrate over t\n",
    "    integral = diff2.sum(dim=-1) * dt  # [B, P]\n",
    "    loss = 0.5 * integral.mean()       # average over batch and P\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b860baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example training loop\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ----- fake data (replace with real functional data) -----\n",
    "n_samples = 256\n",
    "P = 3        # number of functional dimensions\n",
    "T = 100      # number of time points\n",
    "X_train = torch.randn(n_samples, P, T).to(device)\n",
    "\n",
    "dataset = TensorDataset(X_train)\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# ----- build model -----\n",
    "latent_dim = 10\n",
    "num_basis = 16\n",
    "hidden_dims = [32, 32]   # first is size of functional hidden layer\n",
    "\n",
    "model = FunctionalAutoencoder(\n",
    "    P=P,\n",
    "    T=T,\n",
    "    latent_dim=latent_dim,\n",
    "    num_basis=num_basis,\n",
    "    hidden_dims=hidden_dims,\n",
    "    device=device,\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "dt = model.basis.dt\n",
    "\n",
    "# ----- training -----\n",
    "for epoch in range(50):\n",
    "    total_loss = 0.0\n",
    "    for (batch_x,) in loader:\n",
    "        batch_x = batch_x.to(device)\n",
    "\n",
    "        x_hat, z = model(batch_x)\n",
    "        loss = functional_mse_loss(batch_x, x_hat, dt)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * batch_x.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(dataset)\n",
    "    print(f\"Epoch {epoch:03d} | loss = {avg_loss:.6f}\")\n",
    "\n",
    "# After training, get embeddings\n",
    "with torch.no_grad():\n",
    "    embeddings = model.encode(X_train)    # [n_samples, latent_dim]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae934489",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-iris",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
