{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81b7bd3b",
   "metadata": {},
   "source": [
    "**Functional Autoencoder (FAE):** \n",
    "\n",
    "https://github.com/CedricBeaulac/FAE/tree/main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ee2f18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import torch\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import *\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import skfda as fda\n",
    "from skfda import representation as representation\n",
    "from skfda.exploratory.visualization import FPCAPlot\n",
    "import scipy\n",
    "from scipy import stats\n",
    "from scipy.interpolate import BSpline\n",
    "import ignite\n",
    "import os\n",
    "import sklearn\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import random\n",
    "from random import seed\n",
    "import statistics\n",
    "from statistics import stdev\n",
    "from datetime import datetime\n",
    "import matplotlib.ticker as mtick\n",
    "from random import seed\n",
    "\n",
    "# Import functions\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "project_dir = Path(r\"D:\\Mobina\\Marquette\\FAE Codes\")\n",
    "os.chdir(project_dir)\n",
    "if str(project_dir) not in sys.path:\n",
    "    sys.path.insert(0, str(project_dir))\n",
    "\n",
    "import Functions\n",
    "from Functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2283689",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################\n",
    "# Define the vanilla FAE architecture for regularly functional data\n",
    "# Create FAE Class\n",
    "###################################################################\n",
    "# Below are the settings for the creating the linear FAE (adjust the architecure for nonlinear FAE)\n",
    "class FAE_vanilla(nn.Module):\n",
    "    def __init__(self, weight_std=None):\n",
    "        super(FAE_vanilla, self).__init__()\n",
    "        ## Select one of the following options (comment out the other one)\n",
    "        # Opt 1: Linear FAE with 1 hidden layer\n",
    "        self.fc1 = nn.Linear(n_basis_project, n_rep, bias=False)\n",
    "        self.fc3 = nn.Linear(n_rep, n_basis_revert, bias=False)\n",
    "        self.activation = nn.Identity()\n",
    "\n",
    "        # Opt 2: Nonlinear FAE with 3 hidden layers\n",
    "        # self.fc1 = nn.Linear(n_basis_project, 100, bias=False)\n",
    "        # self.fc2 = nn.Linear(100, n_rep, bias=False)\n",
    "        # self.fc3 = nn.Linear(n_rep, 100, bias=False)\n",
    "        # self.fc4 = nn.Linear(100, n_basis_revert, bias=False)\n",
    "        # self.activation = nn.Sigmoid()\n",
    "\n",
    "        # initialize the weights to a specified, constant value\n",
    "        if (weight_std is not None):\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, nn.Linear):\n",
    "                    nn.init.normal_(m.weight, mean=0.0, std=weight_std)\n",
    "                    #nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x, tpts, basis_fc_project, basis_fc_revert):\n",
    "        feature = self.Project(x, tpts, basis_fc_project)\n",
    "\n",
    "        ## Select one of the following options accordingly (comment out the other one)\n",
    "        # Opt 1: Linear FAE with 1 hidden layer\n",
    "        rep = self.activation(self.fc1(feature))\n",
    "        coef = self.fc3(rep)\n",
    "\n",
    "        # Opt 2: Nonlinear inear FAE with 3 hidden layers\n",
    "        # t1 = self.activation(self.fc1(feature))\n",
    "        # rep = self.fc2(t1)\n",
    "        # t2 = self.activation(self.fc3(rep))\n",
    "        # coef = self.fc4(t2)\n",
    "\n",
    "        x_hat = self.Revert(coef, basis_fc_revert)\n",
    "        return x_hat, rep, feature, coef\n",
    "\n",
    "    def Project(self, x, tpts, basis_fc):\n",
    "        # basis_fc: n_time X nbasis\n",
    "        # x: n_subject X n_time\n",
    "        time_diff = tpts[1:] - tpts[:-1]\n",
    "        W = (torch.cat((torch.tensor([[0]]), time_diff))*1/2 + torch.cat((time_diff, torch.tensor([[0]])))*1/2).flatten().float()\n",
    "        f = torch.matmul(torch.mul(x, W), torch.t(basis_fc))\n",
    "        return f\n",
    "\n",
    "    def Revert(self, x, basis_fc):\n",
    "        g = torch.matmul(x, basis_fc)\n",
    "        return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39c8d42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################\n",
    "# Define the training procedure\n",
    "#####################################\n",
    "# The function for training FAE\n",
    "def train(train_loader, pen=None, lamb=0):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    score_loss = 0\n",
    "    for i, data in enumerate(train_loader):\n",
    "        optimizer.zero_grad()  # The gradients are set to zero\n",
    "        input = data.to(device)\n",
    "        out,rep,feature,coef = model(input.float(), tpts, basis_fc_project, basis_fc_revert)\n",
    "        ## Loss on the score layers (network output layer)\n",
    "        if shape(feature)==shape(coef):\n",
    "            score_loss += loss_function(feature, coef) # meaningful when basis functions are orthonormal\n",
    "        # Penalty term\n",
    "        penalty = 0\n",
    "        if pen == \"diff\":\n",
    "            delta_c = coef[:,2:] - 2*coef[:,1:-1] + coef[:,:-2]\n",
    "            penalty = torch.mean(torch.sum(delta_c**2, dim=1))\n",
    "        # Loss for back-propagation\n",
    "        loss = loss_function(out, input.float()) + lamb*penalty\n",
    "        loss.backward()  # The gradient is computed and stored.\n",
    "        optimizer.step()  # Performs parameter update\n",
    "        train_loss += loss\n",
    "    return train_loss, score_loss\n",
    "\n",
    "# The function for predicting observations with trained FAE\n",
    "def pred(model, data):\n",
    "    model.eval()\n",
    "    input = data.to(device)\n",
    "    output, rep, feature, coef = model(input.float(), tpts, basis_fc_project, basis_fc_revert)\n",
    "    loss = loss_function(output, input.float())\n",
    "    return output, rep, loss, coef # score_loss # we comment out score_loss here because it is meaningless in our implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f709c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################\n",
    "# Perform FAE (Model Training)\n",
    "#####################################\n",
    "# Below are the settings for the implementation with ElNino data described in the real application section\n",
    "niter = 20\n",
    "seed(743)\n",
    "niter_seed = random.sample(range(5000), niter)\n",
    "# niter = 10\n",
    "# seed(743)\n",
    "# niter_seed = random.sample(range(1000), niter)\n",
    "\n",
    "# Set up basis functions for input functional weights\n",
    "n_basis_project = 20\n",
    "tpts = time  # or time.numpy() if needed\n",
    "basis_type_project = \"Bspline\"\n",
    "# Get basis functions evaluated\n",
    "if basis_type_project == \"Bspline\":\n",
    "    bss_project = representation.basis.BSpline(n_basis=n_basis_project, order=4)\n",
    "elif basis_type_project == \"Fourier\":\n",
    "    bss_project = representation.basis.Fourier([min(tpts.numpy().flatten()), max(tpts.numpy().flatten())], n_basis=n_basis_project)\n",
    "bss_eval_project = bss_project.evaluate(tpts, derivative=0)\n",
    "basis_fc_project = torch.from_numpy(bss_eval_project[:, :, 0]).float()\n",
    "\n",
    "# Set up basis functions for output functional weights\n",
    "n_basis_revert = 20\n",
    "basis_type_revert = \"Bspline\"\n",
    "# Get basis functions evaluated\n",
    "if basis_type_revert == \"Bspline\":\n",
    "    bss_revert = representation.basis.BSpline(n_basis=n_basis_revert, order=4)\n",
    "elif basis_type_revert == \"Fourier\":\n",
    "    bss_revert = representation.basis.Fourier([min(tpts.numpy().flatten()), max(tpts.numpy().flatten())], n_basis=n_basis_revert)\n",
    "bss_eval_revert = bss_revert.evaluate(tpts, derivative=0)\n",
    "basis_fc_revert = torch.from_numpy(bss_eval_revert[:, :, 0]).float()\n",
    "\n",
    "# Set up lists to save training info\n",
    "FAE_train_no_niter = []\n",
    "FAE_reps_train_niter = []\n",
    "FAE_reps_test_niter = []\n",
    "FAE_reps_all_niter = []\n",
    "FAE_pred_test_niter = []\n",
    "FAE_pred_all_niter = []\n",
    "FAE_coef_train_niter = []\n",
    "FAE_coef_test_niter = []\n",
    "FAE_pred_train_acc_mean_niter = []\n",
    "FAE_pred_test_acc_mean_niter = []\n",
    "FAE_pred_train_acc_sd_niter = []\n",
    "FAE_pred_test_acc_sd_niter = []\n",
    "classification_FAE_train_niter = []\n",
    "classification_FAE_test_niter = []\n",
    "\n",
    "# Set up FAE's hyperparameters\n",
    "n_rep = 5 # number of representation\n",
    "lamb = 0.001 # penalty parameter\n",
    "pen = \"diff\" # penalty type\n",
    "epochs = 5000 # epochs\n",
    "batch_size = 28 # batch size\n",
    "init_weight_sd = 0.5 # SD of normal dist. for initializing NN weight\n",
    "split_rate = 0.8 # percentage of training set\n",
    "\n",
    "# Set up lists for training history\n",
    "FAE_reg_test_acc_epoch = [[] for x in range(int(epochs/100))]\n",
    "classification_FAE_reg_test_epoch = [[] for x in range(int(epochs/100))]\n",
    "\n",
    "# Start iterations\n",
    "for i in range(niter):\n",
    "    # Split training/test set\n",
    "    TrainData, TestData, TrainLabel, TestLabel, train_no = train_test_split(x, label, split_rate =split_rate, seed_no=niter_seed[i])\n",
    "    FAE_train_no_niter.append(train_no)\n",
    "    # Define data loaders; DataLoader is used to load the dataset for training\n",
    "    train_loader = torch.utils.data.DataLoader(TrainData, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(TestData)\n",
    "\n",
    "    # Model Initialization\n",
    "    model = FAE_vanilla(weight_std=init_weight_sd)\n",
    "    loss_function = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-6)\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "    epochs = epochs\n",
    "    # Train model\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        loss, score_loss = train(train_loader=train_loader, pen=pen, lamb=lamb)\n",
    "        FAE_pred_test, FAE_reps_test, FAE_pred_loss_test, FAE_coef_test = pred(model, TestData)\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch[{epoch}]-loss: {loss:.4f}; feature loss: {score_loss: 4f}; pred_loss:{FAE_pred_loss_test:4f}\")\n",
    "            FAE_reg_test_acc_epoch[int(epoch / 100) - 1].append(FAE_pred_loss_test.tolist())\n",
    "            FAE_reps_train_temp = pred(model, TrainData)[1]\n",
    "            FAE_classifier_temp = LogisticRegression(solver='liblinear', random_state=0, multi_class='auto').fit(\n",
    "                FAE_reps_train_temp.detach().numpy(), TrainLabel)\n",
    "            classification_FAE_reg_test_epoch[int(epoch / 100) - 1].append(FAE_classifier_temp.score(FAE_reps_test.detach().numpy(), TestLabel))\n",
    "\n",
    "    FAE_reps_test_niter.append(FAE_reps_test)\n",
    "    FAE_pred_test_niter.append(FAE_pred_test)\n",
    "    FAE_coef_test_niter.append(FAE_coef_test)\n",
    "    FAE_pred_all, FAE_reps_all = pred(model, x)[0:2]\n",
    "    FAE_coef_all = pred(model, x)[3]\n",
    "    FAE_reps_all_niter.append(FAE_reps_all)\n",
    "    FAE_pred_all_niter.append(FAE_pred_all)\n",
    "\n",
    "    FAE_pred_test_acc_mean_niter.append(FAE_pred_loss_test.tolist())\n",
    "    FAE_pred_test_acc_sd_niter.append(eval_mse_sdse(TestData, FAE_pred_test)[1].tolist())\n",
    "\n",
    "    FAE_pred_train, FAE_reps_train, FAE_pred_loss_train, FAE_coef_train = pred(model, TrainData)\n",
    "    FAE_reps_train_niter.append(FAE_reps_train)\n",
    "    FAE_coef_train_niter.append(FAE_coef_train)\n",
    "    FAE_pred_train_acc_mean_niter.append(FAE_pred_loss_train.tolist())\n",
    "    FAE_pred_train_acc_sd_niter.append(eval_mse_sdse(TrainData, FAE_pred_train)[1].tolist())\n",
    "\n",
    "    ## Classification\n",
    "    # Create classifiers (logistic regression) & train the model with the training set\n",
    "    FAE_classifier = LogisticRegression(solver='liblinear', random_state=0, multi_class='auto').fit(FAE_reps_train.detach().numpy(), TrainLabel)\n",
    "    # Classification accuracy on the test set\n",
    "    classification_FAE_test_niter.append(FAE_classifier.score(FAE_reps_test.detach().numpy(), TestLabel))\n",
    "    # Classification accuracy on the training set\n",
    "    classification_FAE_train_niter.append(FAE_classifier.score(FAE_reps_train.detach().numpy(), TrainLabel))\n",
    "\n",
    "    print(f\"Replicate {i+1} is complete.\")\n",
    "\n",
    "# Print for result tables\n",
    "print(\"--- FAE-Nonlinear Results --- \\n\"\n",
    "      f\"Train Pred Acc Mean: {mean(FAE_pred_train_acc_mean_niter[0:20]):.4f}; \"\n",
    "      f\"Train Pred Acc SD: {std(FAE_pred_train_acc_mean_niter[0:20]):.4f}; \\n\"\n",
    "      f\"Test Pred Acc Mean: {mean(FAE_pred_test_acc_mean_niter[0:20]):.4f}; \"\n",
    "      f\"Test Pred Acc SD: {std(FAE_pred_test_acc_mean_niter[0:20]):.4f}; \\n\"\n",
    "      f\"Train Classification Acc Mean: {mean(classification_FAE_train_niter[0:20]):.4f}; \"\n",
    "      f\"Train Classification Acc SD: {std(classification_FAE_train_niter[0:20]):.4f}; \\n\"\n",
    "      f\"Test Classification Acc Mean: {mean(classification_FAE_test_niter[0:20]):.4f}; \"\n",
    "      f\"Test Classification Acc SD: {std(classification_FAE_test_niter[0:20]):.4f}; \\n\")\n",
    "\n",
    "# Paired t-test for prediction error\n",
    "stats.ttest_rel(FAE_pred_test_acc_mean_niter, FPCA_pred_test_acc_mean_niter) # for MSE\n",
    "# Paired t-test for classification accuracy\n",
    "stats.ttest_rel(classification_FAE_test_niter, classification_FPCA_test_niter) # for classification accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-iris",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
