{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81b7bd3b",
   "metadata": {},
   "source": [
    "**Functional Autoencoder (FAE):** \n",
    "\n",
    "https://github.com/CedricBeaulac/FAE/tree/main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ee2f18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import torch\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import *\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import skfda as fda\n",
    "from skfda import representation as representation\n",
    "from skfda.exploratory.visualization import FPCAPlot\n",
    "import scipy\n",
    "from scipy import stats\n",
    "from scipy.interpolate import BSpline\n",
    "import ignite\n",
    "import os\n",
    "import sklearn\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import random\n",
    "from random import seed\n",
    "import statistics\n",
    "from statistics import stdev\n",
    "from datetime import datetime\n",
    "import matplotlib.ticker as mtick\n",
    "from random import seed\n",
    "import random\n",
    "\n",
    "# Import functions\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "project_dir = Path(r\"D:\\Mobina\\Marquette\\FAE Codes\")\n",
    "os.chdir(project_dir)\n",
    "if str(project_dir) not in sys.path:\n",
    "    sys.path.insert(0, str(project_dir))\n",
    "\n",
    "import Functions\n",
    "from Functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09d3c7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read_ElNino_Data.py file for reading the data \n",
    "\"\"\"\n",
    "This script contains the code for importing and pre-processing the simulation data sets\n",
    "in the manuscript \"Functional Autoencoder for Smoothing and Representation Learning\".\n",
    "\n",
    "@author: Sidi Wu\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import *\n",
    "import os\n",
    "import torch\n",
    "\n",
    "os.chdir(r\"D:\\Mobina\\Marquette\\FAE Codes\")\n",
    "\n",
    "#####################################\n",
    "### Real application: ElNino data set\n",
    "#####################################\n",
    "# Import dataset\n",
    "x_raw = pd.read_csv('Dataset/ElNino_ERSST.csv')\n",
    "tpts_raw = pd.read_csv('Dataset/ElNino_ERSST_tpts.csv')\n",
    "label_table = pd.read_csv('Dataset/ElNino_ERSST_label.csv')\n",
    "label = label_table.x.to_numpy()\n",
    "time_grid = np.array(tpts_raw).flatten()\n",
    "\n",
    "# Pre-process Data sets\n",
    "# Prepare numpy/tensor data\n",
    "x_np = np.array(x_raw).astype(float)\n",
    "x = torch.tensor(x_np).float()\n",
    "x_mean = torch.mean(x,0)\n",
    "x = x - torch.mean(x,0)\n",
    "\n",
    "# Rescale timestamp to [0,1]\n",
    "tpts_np = np.array(tpts_raw)\n",
    "tpts_rescale = (tpts_np - min(tpts_np)) / np.ptp(tpts_np)\n",
    "tpts = torch.tensor(np.array(tpts_rescale))\n",
    "n_tpts = len(tpts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2283689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== Replicate 1/20 ==========\n",
      "Epoch 500: train_loss=0.6685, test_loss=0.0828\n",
      "Epoch 1000: train_loss=0.3144, test_loss=0.0369\n",
      "Epoch 1500: train_loss=0.2054, test_loss=0.0211\n",
      "Epoch 2000: train_loss=0.1690, test_loss=0.0183\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Comparison of Linear Functional Autoencoder (FAE) and FPCA (implemented as PCA)\n",
    "on the El Niño data, using the ideas from\n",
    "\"Functional Autoencoder for Smoothing and Representation Learning\".\n",
    "\n",
    "This version:\n",
    "- Uses a purely linear autoencoder (no basis functions, no nonlinearity)\n",
    "- Uses PCA on the same centered data as FPCA\n",
    "- Provides side-by-side comparison (MSE + classification accuracy)\n",
    "\"\"\"\n",
    "\n",
    "# ============================\n",
    "# Imports\n",
    "# ============================\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy import stats\n",
    "import random\n",
    "\n",
    "# ============================\n",
    "# 1. Load and preprocess data\n",
    "# ============================\n",
    "\n",
    "# --- Load data just like in your original script ---\n",
    "os.chdir(r\"D:\\Mobina\\Marquette\\FAE Codes\")\n",
    "\n",
    "# Import dataset\n",
    "x_raw = pd.read_csv('Dataset/ElNino_ERSST.csv')\n",
    "tpts_raw = pd.read_csv('Dataset/ElNino_ERSST_tpts.csv')\n",
    "label_table = pd.read_csv('Dataset/ElNino_ERSST_label.csv')\n",
    "\n",
    "label = label_table.x.to_numpy()\n",
    "time_grid = np.array(tpts_raw).flatten()\n",
    "\n",
    "# N × T matrix\n",
    "x_np = np.array(x_raw).astype(float)\n",
    "x = torch.tensor(x_np).float()  # N × T\n",
    "\n",
    "# Center data (very important for PCA/AE equivalence)\n",
    "x_mean = torch.mean(x, dim=0)\n",
    "x_centered = x - x_mean\n",
    "\n",
    "# Also keep a numpy version\n",
    "x_centered_np = x_centered.numpy()\n",
    "\n",
    "n_subjects, n_time = x_centered_np.shape\n",
    "\n",
    "# ============================\n",
    "# 2. Utility functions\n",
    "# ============================\n",
    "\n",
    "def split_data(x_tensor, labels, split_rate=0.8, seed_no=0):\n",
    "    \"\"\"\n",
    "    Split into train/test, return tensors and index of train subjects.\n",
    "    \"\"\"\n",
    "    N = x_tensor.shape[0]\n",
    "    idx = np.arange(N)\n",
    "    train_idx, test_idx = train_test_split(\n",
    "        idx, train_size=split_rate, random_state=seed_no, shuffle=True\n",
    "    )\n",
    "    TrainData = x_tensor[train_idx]\n",
    "    TestData = x_tensor[test_idx]\n",
    "    TrainLabel = labels[train_idx]\n",
    "    TestLabel = labels[test_idx]\n",
    "    return TrainData, TestData, TrainLabel, TestLabel, train_idx\n",
    "\n",
    "def eval_mse_sdse(true_tensor, pred_tensor):\n",
    "    \"\"\"\n",
    "    Compute:\n",
    "    - Mean MSE over all subjects\n",
    "    - Standard deviation of curve-wise MSEs\n",
    "    \"\"\"\n",
    "    true_np = true_tensor.numpy()\n",
    "    pred_np = pred_tensor.numpy()\n",
    "    # Curve-wise MSE\n",
    "    mse_curves = np.mean((true_np - pred_np) ** 2, axis=1)  # length N\n",
    "    mse_mean = mse_curves.mean()\n",
    "    mse_sd = mse_curves.std()\n",
    "    return mse_mean, mse_sd\n",
    "\n",
    "# ============================\n",
    "# 3. Linear Autoencoder (FAE)\n",
    "# ============================\n",
    "\n",
    "class LinearFAE(nn.Module):\n",
    "    \"\"\"\n",
    "    Purely linear autoencoder:\n",
    "    - Encoder: Linear(input_dim -> n_rep), no bias (to match PCA more closely)\n",
    "    - Decoder: Linear(n_rep -> input_dim), no bias\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, n_rep, weight_std=None):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Linear(input_dim, n_rep, bias=False)\n",
    "        self.decoder = nn.Linear(n_rep, input_dim, bias=False)\n",
    "        self.activation = nn.Identity()\n",
    "\n",
    "        if weight_std is not None:\n",
    "            nn.init.normal_(self.encoder.weight, mean=0.0, std=weight_std)\n",
    "            nn.init.normal_(self.decoder.weight, mean=0.0, std=weight_std)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: N × T\n",
    "        rep = self.activation(self.encoder(x))  # N × n_rep\n",
    "        out = self.decoder(rep)                 # N × T\n",
    "        return out, rep\n",
    "\n",
    "def train_fae(model, train_loader, optimizer, loss_function, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(device).float()\n",
    "        optimizer.zero_grad()\n",
    "        x_hat, rep = model(batch)\n",
    "        loss = loss_function(x_hat, batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss\n",
    "\n",
    "def pred_fae(model, data_tensor, loss_function, device):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        data = data_tensor.to(device).float()\n",
    "        out, rep = model(data)\n",
    "        loss = loss_function(out, data)\n",
    "    return out.cpu(), rep.cpu(), loss.item()\n",
    "\n",
    "# ============================\n",
    "# 4. FPCA as PCA on grid data\n",
    "# ============================\n",
    "\n",
    "def fpca_fit_predict(TrainData, TestData, n_rep):\n",
    "    \"\"\"\n",
    "    FPCA implemented as PCA on discretized curves (N × T).\n",
    "    \"\"\"\n",
    "    Train_np = TrainData.numpy()\n",
    "    Test_np = TestData.numpy()\n",
    "\n",
    "    pca = PCA(n_components=n_rep)\n",
    "    pca.fit(Train_np)\n",
    "\n",
    "    # scores\n",
    "    scores_train = pca.transform(Train_np)   # N_train × n_rep\n",
    "    scores_test = pca.transform(Test_np)     # N_test × n_rep\n",
    "    scores_all = pca.transform(x_centered_np)\n",
    "\n",
    "    # reconstructions\n",
    "    pred_train = pca.inverse_transform(scores_train)\n",
    "    pred_test = pca.inverse_transform(scores_test)\n",
    "    pred_all = pca.inverse_transform(scores_all)\n",
    "\n",
    "    # back to torch tensors\n",
    "    pred_train_t = torch.tensor(pred_train).float()\n",
    "    pred_test_t = torch.tensor(pred_test).float()\n",
    "    pred_all_t = torch.tensor(pred_all).float()\n",
    "\n",
    "    return (pca,\n",
    "            scores_train, scores_test, scores_all,\n",
    "            pred_train_t, pred_test_t, pred_all_t)\n",
    "\n",
    "# ============================\n",
    "# 5. Experiment setup\n",
    "# ============================\n",
    "\n",
    "niter = 20\n",
    "random.seed(743)\n",
    "niter_seed = random.sample(range(5000), niter)\n",
    "\n",
    "n_rep = 5            # latent dimension\n",
    "epochs = 3000        # can reduce if too slow\n",
    "batch_size = 28\n",
    "init_weight_sd = 0.5\n",
    "split_rate = 0.8\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "# --- Storage for results ---\n",
    "FAE_pred_train_acc_mean_niter = []\n",
    "FAE_pred_train_acc_sd_niter = []\n",
    "FAE_pred_test_acc_mean_niter = []\n",
    "FAE_pred_test_acc_sd_niter = []\n",
    "classification_FAE_train_niter = []\n",
    "classification_FAE_test_niter = []\n",
    "\n",
    "FPCA_pred_train_acc_mean_niter = []\n",
    "FPCA_pred_train_acc_sd_niter = []\n",
    "FPCA_pred_test_acc_mean_niter = []\n",
    "FPCA_pred_test_acc_sd_niter = []\n",
    "classification_FPCA_train_niter = []\n",
    "classification_FPCA_test_niter = []\n",
    "\n",
    "# To compare representations more explicitly (optional)\n",
    "corr_first_component_niter = []\n",
    "\n",
    "# ============================\n",
    "# 6. Main loop over replicates\n",
    "# ============================\n",
    "\n",
    "for i in range(niter):\n",
    "    print(f\"\\n========== Replicate {i+1}/{niter} ==========\")\n",
    "    seed_no = niter_seed[i]\n",
    "\n",
    "    # ------------------------\n",
    "    # Data split\n",
    "    # ------------------------\n",
    "    TrainData, TestData, TrainLabel, TestLabel, train_no = split_data(\n",
    "        x_centered, label, split_rate=split_rate, seed_no=seed_no\n",
    "    )\n",
    "\n",
    "    # ------------------------\n",
    "    # 6a. Train Linear FAE\n",
    "    # ------------------------\n",
    "    model = LinearFAE(input_dim=n_time, n_rep=n_rep, weight_std=init_weight_sd)\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-6)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        TrainData, batch_size=batch_size, shuffle=True\n",
    "    )\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        loss_epoch = train_fae(\n",
    "            model=model,\n",
    "            train_loader=train_loader,\n",
    "            optimizer=optimizer,\n",
    "            loss_function=loss_function,\n",
    "            device=device,\n",
    "        )\n",
    "        if epoch % 500 == 0:\n",
    "            # quick monitoring on test loss\n",
    "            fae_pred_test, fae_reps_test, fae_test_loss, _ = None, None, None, None\n",
    "            fae_pred_test, fae_reps_test, fae_test_loss = pred_fae(\n",
    "                model, TestData, loss_function, device\n",
    "            )[:3]\n",
    "            print(f\"Epoch {epoch}: train_loss={loss_epoch:.4f}, test_loss={fae_test_loss:.4f}\")\n",
    "\n",
    "    # final predictions\n",
    "    FAE_pred_train, FAE_reps_train, FAE_train_loss = pred_fae(\n",
    "        model, TrainData, loss_function, device\n",
    "    )\n",
    "    FAE_pred_test, FAE_reps_test, FAE_test_loss = pred_fae(\n",
    "        model, TestData, loss_function, device\n",
    "    )\n",
    "    FAE_pred_all, FAE_reps_all, _ = pred_fae(\n",
    "        model, x_centered, loss_function, device\n",
    "    )\n",
    "\n",
    "    # prediction accuracy (MSE)\n",
    "    fae_train_mse_mean, fae_train_mse_sd = eval_mse_sdse(TrainData, FAE_pred_train)\n",
    "    fae_test_mse_mean, fae_test_mse_sd = eval_mse_sdse(TestData, FAE_pred_test)\n",
    "\n",
    "    FAE_pred_train_acc_mean_niter.append(fae_train_mse_mean)\n",
    "    FAE_pred_train_acc_sd_niter.append(fae_train_mse_sd)\n",
    "    FAE_pred_test_acc_mean_niter.append(fae_test_mse_mean)\n",
    "    FAE_pred_test_acc_sd_niter.append(fae_test_mse_sd)\n",
    "\n",
    "    # Classification using FAE representations\n",
    "    FAE_classifier = LogisticRegression(\n",
    "        solver='liblinear', random_state=0, multi_class='auto'\n",
    "    ).fit(FAE_reps_train.numpy(), TrainLabel)\n",
    "\n",
    "    classification_FAE_train_niter.append(\n",
    "        FAE_classifier.score(FAE_reps_train.numpy(), TrainLabel)\n",
    "    )\n",
    "    classification_FAE_test_niter.append(\n",
    "        FAE_classifier.score(FAE_reps_test.numpy(), TestLabel)\n",
    "    )\n",
    "\n",
    "    # ------------------------\n",
    "    # 6b. FPCA as PCA on grid\n",
    "    # ------------------------\n",
    "    (fpca_model,\n",
    "     fpc_scores_train, fpc_scores_test, fpc_scores_all,\n",
    "     FPCA_pred_train, FPCA_pred_test, FPCA_pred_all) = fpca_fit_predict(\n",
    "        TrainData, TestData, n_rep=n_rep\n",
    "    )\n",
    "\n",
    "    # prediction accuracy (MSE)\n",
    "    fpca_train_mse_mean, fpca_train_mse_sd = eval_mse_sdse(TrainData, FPCA_pred_train)\n",
    "    fpca_test_mse_mean, fpca_test_mse_sd = eval_mse_sdse(TestData, FPCA_pred_test)\n",
    "\n",
    "    FPCA_pred_train_acc_mean_niter.append(fpca_train_mse_mean)\n",
    "    FPCA_pred_train_acc_sd_niter.append(fpca_train_mse_sd)\n",
    "    FPCA_pred_test_acc_mean_niter.append(fpca_test_mse_mean)\n",
    "    FPCA_pred_test_acc_sd_niter.append(fpca_test_mse_sd)\n",
    "\n",
    "    # Classification using FPCA scores\n",
    "    FPCA_classifier = LogisticRegression(\n",
    "        solver='liblinear', random_state=0, multi_class='auto'\n",
    "    ).fit(fpc_scores_train, TrainLabel)\n",
    "\n",
    "    classification_FPCA_train_niter.append(\n",
    "        FPCA_classifier.score(fpc_scores_train, TrainLabel)\n",
    "    )\n",
    "    classification_FPCA_test_niter.append(\n",
    "        FPCA_classifier.score(fpc_scores_test, TestLabel)\n",
    "    )\n",
    "\n",
    "    # ------------------------\n",
    "    # 6c. Representation comparison (optional)\n",
    "    # Compare first FAE component vs first FPCA component\n",
    "    # (They are only identifiable up to rotation and sign,\n",
    "    #  so correlation ~ ±1 indicates strong equivalence.)\n",
    "    # Use all subjects:\n",
    "    fae_first = FAE_reps_all.numpy()[:, 0]\n",
    "    fpca_first = fpc_scores_all[:, 0]\n",
    "    corr = np.corrcoef(fae_first, fpca_first)[0, 1]\n",
    "    corr_first_component_niter.append(corr)\n",
    "    print(f\"Correlation between first FAE and FPCA component: {corr:.4f}\")\n",
    "\n",
    "print(\"\\n================ SUMMARY ================\\n\")\n",
    "\n",
    "# ============================\n",
    "# 7. Aggregate results & stats\n",
    "# ============================\n",
    "\n",
    "FAE_train_mse_mean = np.mean(FAE_pred_train_acc_mean_niter)\n",
    "FAE_train_mse_sd   = np.std(FAE_pred_train_acc_mean_niter)\n",
    "FAE_test_mse_mean  = np.mean(FAE_pred_test_acc_mean_niter)\n",
    "FAE_test_mse_sd    = np.std(FAE_pred_test_acc_mean_niter)\n",
    "\n",
    "FPCA_train_mse_mean = np.mean(FPCA_pred_train_acc_mean_niter)\n",
    "FPCA_train_mse_sd   = np.std(FPCA_pred_train_acc_mean_niter)\n",
    "FPCA_test_mse_mean  = np.mean(FPCA_pred_test_acc_mean_niter)\n",
    "FPCA_test_mse_sd    = np.std(FPCA_pred_test_acc_mean_niter)\n",
    "\n",
    "FAE_train_acc_mean = np.mean(classification_FAE_train_niter)\n",
    "FAE_train_acc_sd   = np.std(classification_FAE_train_niter)\n",
    "FAE_test_acc_mean  = np.mean(classification_FAE_test_niter)\n",
    "FAE_test_acc_sd    = np.std(classification_FAE_test_niter)\n",
    "\n",
    "FPCA_train_acc_mean = np.mean(classification_FPCA_train_niter)\n",
    "FPCA_train_acc_sd   = np.std(classification_FPCA_train_niter)\n",
    "FPCA_test_acc_mean  = np.mean(classification_FPCA_test_niter)\n",
    "FPCA_test_acc_sd    = np.std(classification_FPCA_test_niter)\n",
    "\n",
    "print(\"--- Linear FAE Results ---\")\n",
    "print(f\"Train MSE Mean: {FAE_train_mse_mean:.4f}; Train MSE SD: {FAE_train_mse_sd:.4f}\")\n",
    "print(f\"Test  MSE Mean: {FAE_test_mse_mean:.4f}; Test  MSE SD: {FAE_test_mse_sd:.4f}\")\n",
    "print(f\"Train Classification Acc Mean: {FAE_train_acc_mean:.4f}; SD: {FAE_train_acc_sd:.4f}\")\n",
    "print(f\"Test  Classification Acc Mean: {FAE_test_acc_mean:.4f}; SD: {FAE_test_acc_sd:.4f}\\n\")\n",
    "\n",
    "print(\"--- FPCA (PCA) Results ---\")\n",
    "print(f\"Train MSE Mean: {FPCA_train_mse_mean:.4f}; Train MSE SD: {FPCA_train_mse_sd:.4f}\")\n",
    "print(f\"Test  MSE Mean: {FPCA_test_mse_mean:.4f}; Test  MSE SD: {FPCA_test_mse_sd:.4f}\")\n",
    "print(f\"Train Classification Acc Mean: {FPCA_train_acc_mean:.4f}; SD: {FPCA_train_acc_sd:.4f}\")\n",
    "print(f\"Test  Classification Acc Mean: {FPCA_test_acc_mean:.4f}; SD: {FPCA_test_acc_sd:.4f}\\n\")\n",
    "\n",
    "# Paired t-tests (per replicate)\n",
    "print(\"--- Paired t-tests (per replicate) ---\")\n",
    "print(\"Prediction error (MSE) on test:\")\n",
    "tt_mse = stats.ttest_rel(FAE_pred_test_acc_mean_niter, FPCA_pred_test_acc_mean_niter)\n",
    "print(f\"t = {tt_mse.statistic:.4f}, p = {tt_mse.pvalue:.4e}\")\n",
    "\n",
    "print(\"Classification accuracy on test:\")\n",
    "tt_acc = stats.ttest_rel(classification_FAE_test_niter, classification_FPCA_test_niter)\n",
    "print(f\"t = {tt_acc.statistic:.4f}, p = {tt_acc.pvalue:.4e}\")\n",
    "\n",
    "print(\"\\nMean correlation between first FAE and FPCA component across replicates:\")\n",
    "print(f\"{np.mean(corr_first_component_niter):.4f} (SD {np.std(corr_first_component_niter):.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7523de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-iris",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
