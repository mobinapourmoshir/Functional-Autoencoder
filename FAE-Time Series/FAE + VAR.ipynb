{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3335b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch   200 | train_loss=0.003987\n",
      "epoch   400 | train_loss=0.002913\n",
      "epoch   600 | train_loss=0.001894\n",
      "epoch   800 | train_loss=0.001759\n",
      "epoch  1000 | train_loss=0.001421\n",
      "epoch  1200 | train_loss=0.001896\n",
      "epoch  1400 | train_loss=0.000920\n",
      "epoch  1600 | train_loss=0.000975\n",
      "epoch  1800 | train_loss=0.000774\n",
      "epoch  2000 | train_loss=0.000663\n",
      "epoch  2200 | train_loss=0.000556\n",
      "epoch  2400 | train_loss=0.000353\n",
      "epoch  2600 | train_loss=0.001921\n",
      "epoch  2800 | train_loss=0.000392\n",
      "epoch  3000 | train_loss=0.000286\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Functional Autoencoder (FAE) + Latent VAR Forecasting\n",
    "# Australia Fertility Data (1921–2015)\n",
    "#\n",
    "# Goal:\n",
    "#  1) Train a functional autoencoder on curves up to year 2010\n",
    "#  2) Treat the latent vectors h_t as a multivariate time series\n",
    "#  3) Fit a VAR model to {h_t}\n",
    "#  4) Forecast h_2011..h_2015\n",
    "#  5) Decode forecast latents back to curves\n",
    "#  6) Plot Real vs Forecast for 2010–2015\n",
    "#\n",
    "# Notes:\n",
    "#  - Uses B-spline basis (or Fourier) evaluated on rescaled age grid.\n",
    "#  - Projects curves to basis coefficients via weighted least squares (stable for splines).\n",
    "#  - Trains AE in coefficient space, but reconstruction loss is measured in curve space.\n",
    "#  - Standardizes curves by age (mean/std across years) for stable training and VAR.\n",
    "#  - Limits BLAS/MKL threads to avoid kernel crashes on some systems.\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "# ---- thread limits to reduce kernel crashes (set BEFORE numpy/torch) ----\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from statsmodels.tsa.api import VAR  # pip install statsmodels\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Reproducibility\n",
    "# -------------------------\n",
    "def set_seed(seed=743):\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Basis construction\n",
    "# -------------------------\n",
    "class BasisFCBuilder:\n",
    "    \"\"\"\n",
    "    Build basis matrix evaluated on tpts.\n",
    "    Output shape: [n_time, n_basis]\n",
    "    \"\"\"\n",
    "    def __init__(self, n_basis=20, basis_type=\"Bspline\", custom_basis_fn=None, bspline_degree=3):\n",
    "        self.n_basis = n_basis\n",
    "        self.basis_type = basis_type\n",
    "        self.basis_type_l = basis_type.lower()\n",
    "        self.custom_basis_fn = custom_basis_fn\n",
    "        self.bspline_degree = bspline_degree\n",
    "\n",
    "    def build(self, tpts: torch.Tensor):\n",
    "        if self.custom_basis_fn is not None:\n",
    "            B = self.custom_basis_fn(tpts)\n",
    "            if not torch.is_tensor(B):\n",
    "                B = torch.tensor(B, dtype=torch.float32)\n",
    "            return B.float()\n",
    "\n",
    "        if self.basis_type_l == \"fourier\":\n",
    "            return self._build_fourier(tpts)\n",
    "        elif self.basis_type_l in (\"bspline\", \"b-spline\", \"b_spline\"):\n",
    "            return self._build_bspline(tpts, degree=self.bspline_degree)\n",
    "        else:\n",
    "            raise ValueError(\"basis_type must be 'Fourier' or 'Bspline'.\")\n",
    "\n",
    "    def _build_fourier(self, tpts: torch.Tensor):\n",
    "        t = tpts.flatten()\n",
    "        t_min, t_max = t.min(), t.max()\n",
    "        denom = (t_max - t_min).clamp_min(1e-8)\n",
    "        tau = (t - t_min) / denom  # normalize to [0,1]\n",
    "\n",
    "        n_time = tpts.shape[0]\n",
    "        n_basis = self.n_basis\n",
    "        device = t.device\n",
    "\n",
    "        B = torch.zeros(n_time, n_basis, dtype=torch.float32, device=device)\n",
    "        if n_basis > 0:\n",
    "            B[:, 0] = 1.0\n",
    "\n",
    "        k = 1\n",
    "        idx = 1\n",
    "        while idx < n_basis:\n",
    "            B[:, idx] = torch.sin(2.0 * math.pi * k * tau)\n",
    "            idx += 1\n",
    "            if idx < n_basis:\n",
    "                B[:, idx] = torch.cos(2.0 * math.pi * k * tau)\n",
    "                idx += 1\n",
    "            k += 1\n",
    "        return B\n",
    "\n",
    "    def _build_bspline(self, tpts: torch.Tensor, degree: int = 3):\n",
    "        # Open-uniform B-spline basis on [0,1] built via Cox-de Boor recursion\n",
    "        t = tpts.flatten()\n",
    "        t_min, t_max = t.min(), t.max()\n",
    "        denom = (t_max - t_min).clamp_min(1e-8)\n",
    "        tau = (t - t_min) / denom\n",
    "        tau_np = tau.detach().cpu().numpy()\n",
    "\n",
    "        n_time = tau_np.shape[0]\n",
    "        n_basis = self.n_basis\n",
    "        p = degree\n",
    "\n",
    "        if n_basis < p + 1:\n",
    "            raise ValueError(f\"Bspline: n_basis={n_basis} must be at least degree+1={p+1}.\")\n",
    "\n",
    "        n_int = max(n_basis - p - 1, 0)\n",
    "        if n_int > 0:\n",
    "            interior = np.linspace(0.0, 1.0, n_int + 2)[1:-1]\n",
    "            knots = np.concatenate((np.zeros(p + 1), interior, np.ones(p + 1)))\n",
    "        else:\n",
    "            knots = np.concatenate((np.zeros(p + 1), np.ones(p + 1)))\n",
    "\n",
    "        N = np.zeros((n_basis, n_time), dtype=np.float64)\n",
    "\n",
    "        # degree 0\n",
    "        for i in range(n_basis):\n",
    "            left, right = knots[i], knots[i + 1]\n",
    "            N[i, :] = np.where((tau_np >= left) & (tau_np < right), 1.0, 0.0)\n",
    "        N[-1, tau_np == 1.0] = 1.0\n",
    "\n",
    "        # elevate to degree p\n",
    "        for k in range(1, p + 1):\n",
    "            N_next = np.zeros_like(N)\n",
    "            for i in range(n_basis):\n",
    "                denom_left = knots[i + k] - knots[i]\n",
    "                if denom_left > 0:\n",
    "                    coeff_left = (tau_np - knots[i]) / denom_left\n",
    "                    N_left = coeff_left * N[i, :]\n",
    "                else:\n",
    "                    N_left = 0.0\n",
    "\n",
    "                denom_right = (knots[i + k + 1] - knots[i + 1]) if (i + 1) < n_basis else 0.0\n",
    "                if denom_right > 0 and (i + 1) < n_basis:\n",
    "                    coeff_right = (knots[i + k + 1] - tau_np) / denom_right\n",
    "                    N_right = coeff_right * N[i + 1, :]\n",
    "                else:\n",
    "                    N_right = 0.0\n",
    "\n",
    "                N_next[i, :] = N_left + N_right\n",
    "            N = N_next\n",
    "\n",
    "        return torch.tensor(N.T, dtype=torch.float32, device=tpts.device)  # [n_time, n_basis]\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Weighted LS projection to basis coefficients\n",
    "# -------------------------\n",
    "def trapezoid_weights(tpts: torch.Tensor):\n",
    "    \"\"\"Trapezoidal weights W_vec, shape [n_time].\"\"\"\n",
    "    t = tpts.flatten()\n",
    "    dt = t[1:] - t[:-1]\n",
    "    zero = torch.zeros(1, device=tpts.device, dtype=tpts.dtype)\n",
    "    W = 0.5 * torch.cat([zero, dt]) + 0.5 * torch.cat([dt, zero])\n",
    "    return W\n",
    "\n",
    "def project_to_basis_coeffs(x: torch.Tensor, B: torch.Tensor, W_vec: torch.Tensor, ridge=1e-6):\n",
    "    \"\"\"\n",
    "    Weighted least squares projection:\n",
    "      c = (B^T W B + ridge I)^(-1) (B^T W x)\n",
    "    x: [N, n_time]\n",
    "    B: [n_time, n_basis]\n",
    "    returns c: [N, n_basis]\n",
    "    \"\"\"\n",
    "    BW = B * W_vec[:, None]                 # [T, M]\n",
    "    G = B.T @ BW                            # [M, M]\n",
    "    M = G.shape[0]\n",
    "    G = G + ridge * torch.eye(M, device=G.device, dtype=G.dtype)\n",
    "    Ginv = torch.linalg.pinv(G)             # stable inverse\n",
    "\n",
    "    xW = x * W_vec[None, :]                 # [N, T]\n",
    "    rhs = (B.T @ xW.T).T                    # [N, M]\n",
    "    c = rhs @ Ginv.T                        # [N, M]\n",
    "    return c\n",
    "\n",
    "def reconstruct_from_coeffs(c: torch.Tensor, B: torch.Tensor):\n",
    "    \"\"\"x_hat = c B^T; c [N,M], B [T,M] -> [N,T]\"\"\"\n",
    "    return c @ B.T\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Functional autoencoder in coefficient space\n",
    "# -------------------------\n",
    "class FAECoef(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder: coeffs -> latent h (K)\n",
    "    Decoder: latent h -> coeffs_hat\n",
    "    Curve reconstruction: x_hat = coeffs_hat @ B^T\n",
    "    \"\"\"\n",
    "    def __init__(self, n_basis: int, n_rep: int, hidden: int = 64, nonlinear: bool = True):\n",
    "        super().__init__()\n",
    "        act = nn.Tanh() if nonlinear else nn.Identity()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(n_basis, hidden),\n",
    "            act,\n",
    "            nn.Linear(hidden, n_rep)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(n_rep, hidden),\n",
    "            act,\n",
    "            nn.Linear(hidden, n_basis)\n",
    "        )\n",
    "\n",
    "    def forward(self, coef):\n",
    "        h = self.encoder(coef)\n",
    "        coef_hat = self.decoder(h)\n",
    "        return coef_hat, h\n",
    "\n",
    "\n",
    "def curve_smooth_penalty(x_hat):\n",
    "    \"\"\"Second finite difference penalty on curves x_hat [N,T].\"\"\"\n",
    "    d2 = x_hat[:, 2:] - 2 * x_hat[:, 1:-1] + x_hat[:, :-2]\n",
    "    return (d2**2).mean()\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Train / encode / decode\n",
    "# -------------------------\n",
    "def train_fae(\n",
    "    coef_train: torch.Tensor,\n",
    "    x_train: torch.Tensor,\n",
    "    B: torch.Tensor,\n",
    "    n_rep: int = 7,\n",
    "    hidden: int = 64,\n",
    "    nonlinear: bool = True,\n",
    "    epochs: int = 3000,\n",
    "    batch_size: int = 16,\n",
    "    lr: float = 2e-3,\n",
    "    lamb: float = 1e-4,\n",
    "    device: str = \"cpu\",\n",
    "    log_every: int = 200\n",
    "):\n",
    "    \"\"\"\n",
    "    Train AE on (coef, x) pairs.\n",
    "    Loss compares reconstructed curves to the true curves.\n",
    "    \"\"\"\n",
    "    device = torch.device(device)\n",
    "    coef_train = coef_train.to(device)\n",
    "    x_train = x_train.to(device)\n",
    "    B = B.to(device)\n",
    "\n",
    "    ds = TensorDataset(coef_train, x_train)\n",
    "    loader = DataLoader(ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    model = FAECoef(n_basis=coef_train.shape[1], n_rep=n_rep, hidden=hidden, nonlinear=nonlinear).to(device)\n",
    "    opt = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    history = {\"epoch\": [], \"train_loss\": []}\n",
    "\n",
    "    for ep in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        tot = 0.0\n",
    "        nb = 0\n",
    "\n",
    "        for coef_b, x_b in loader:\n",
    "            coef_hat, _ = model(coef_b)\n",
    "            x_hat = reconstruct_from_coeffs(coef_hat, B)\n",
    "\n",
    "            loss = loss_fn(x_hat, x_b)\n",
    "            if lamb > 0:\n",
    "                loss = loss + lamb * curve_smooth_penalty(x_hat)\n",
    "\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            opt.step()\n",
    "\n",
    "            tot += float(loss.detach().cpu())\n",
    "            nb += 1\n",
    "\n",
    "        if ep % log_every == 0:\n",
    "            avg = tot / max(nb, 1)\n",
    "            history[\"epoch\"].append(ep)\n",
    "            history[\"train_loss\"].append(avg)\n",
    "            print(f\"epoch {ep:5d} | train_loss={avg:.6f}\")\n",
    "\n",
    "    return model, history\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def encode_latents(model: nn.Module, coef: torch.Tensor, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    coef = coef.to(device)\n",
    "    _, h = model(coef)\n",
    "    return h.detach().cpu().numpy()\n",
    "\n",
    "@torch.no_grad()\n",
    "def decode_latents_to_curves(model: nn.Module, h_np: np.ndarray, B: torch.Tensor, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    h = torch.tensor(h_np, dtype=torch.float32, device=device)\n",
    "    coef_hat = model.decoder(h)\n",
    "    x_hat = reconstruct_from_coeffs(coef_hat, B.to(device))\n",
    "    return x_hat.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# VAR latent forecasting\n",
    "# -------------------------\n",
    "def fit_var_forecast(h_train: np.ndarray, steps: int = 5, maxlags: int = 2):\n",
    "    \"\"\"\n",
    "    h_train: [T_train, K]\n",
    "    returns: h_fore [steps, K], fitted var results\n",
    "    \"\"\"\n",
    "    h_train = np.asarray(h_train)\n",
    "    if not np.isfinite(h_train).all():\n",
    "        raise ValueError(\"Latents contain NaN/Inf. Training or scaling unstable.\")\n",
    "\n",
    "    var_model = VAR(h_train)\n",
    "    res = var_model.fit(maxlags=maxlags, ic=None)  # keep it stable\n",
    "    h_fore = res.forecast(h_train[-res.k_ar:], steps=steps)\n",
    "    return h_fore, res\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Plotting\n",
    "# -------------------------\n",
    "def plot_training_loss(history):\n",
    "    if len(history[\"epoch\"]) == 0:\n",
    "        return\n",
    "    plt.figure()\n",
    "    plt.plot(history[\"epoch\"], history[\"train_loss\"], marker=\"o\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Train loss (MSE)\")\n",
    "    plt.title(\"FAE training loss\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_real_vs_forecast(ages, years, x_real, forecast_years, x_fore, title):\n",
    "    \"\"\"\n",
    "    x_real: [n_year, n_age] in original scale\n",
    "    x_fore: [len(forecast_years), n_age] in original scale\n",
    "    Plots 2010-2015 real and 2011-2015 forecast dashed.\n",
    "    \"\"\"\n",
    "    year_to_idx = {y: i for i, y in enumerate(years)}\n",
    "    plt.figure(figsize=(9, 5))\n",
    "\n",
    "    # plot real 2010-2015\n",
    "    for y in [2010, 2011, 2012, 2013, 2014, 2015]:\n",
    "        if y in year_to_idx:\n",
    "            plt.plot(ages, x_real[year_to_idx[y]], label=f\"Real {y}\")\n",
    "\n",
    "    # plot forecasts\n",
    "    for i, y in enumerate(forecast_years):\n",
    "        plt.plot(ages, x_fore[i], \"--\", label=f\"Forecast {y}\")\n",
    "\n",
    "    plt.xlabel(\"Age\")\n",
    "    plt.ylabel(\"Fertility rate\")\n",
    "    plt.title(title)\n",
    "    plt.legend(ncol=2, fontsize=8)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# MAIN\n",
    "# ============================================================\n",
    "if __name__ == \"__main__\":\n",
    "    set_seed(743)\n",
    "    torch.set_num_threads(1)\n",
    "\n",
    "    # -------- Load data\n",
    "    df = pd.read_csv(\"Australiafertility.csv\")\n",
    "    ages = df[\"age\"].to_numpy()\n",
    "\n",
    "    year_cols = [c for c in df.columns if c != \"age\"]\n",
    "    years = np.array([int(c) for c in year_cols])\n",
    "\n",
    "    rates = df[year_cols].to_numpy()        # [n_age, n_year]\n",
    "    x_raw = rates.T.astype(float)           # [n_year, n_age]\n",
    "    # If you have smoothed curves, use:\n",
    "    # x_raw = smoothed.T.astype(float)\n",
    "\n",
    "    # -------- Age grid -> [0,1]\n",
    "    tpts_np = ages.astype(float)\n",
    "    tpts_rescale = (tpts_np - tpts_np.min()) / (tpts_np.max() - tpts_np.min())\n",
    "    tpts = torch.tensor(tpts_rescale, dtype=torch.float32)\n",
    "\n",
    "    # -------- Basis\n",
    "    n_basis = 50\n",
    "    basis_type = \"Bspline\"   # or \"Fourier\"\n",
    "    bspline_degree = 3\n",
    "\n",
    "    basis_builder = BasisFCBuilder(n_basis=n_basis, basis_type=basis_type, bspline_degree=bspline_degree)\n",
    "    B = basis_builder.build(tpts)           # [n_age, n_basis]\n",
    "    W_vec = trapezoid_weights(tpts)         # [n_age]\n",
    "\n",
    "    # -------- Convert to tensor\n",
    "    x = torch.tensor(x_raw, dtype=torch.float32)  # [n_year, n_age]\n",
    "\n",
    "    # -------- Standardize by age (mean/std over years)\n",
    "    x_mean = x.mean(dim=0, keepdim=True)\n",
    "    x_std = x.std(dim=0, keepdim=True).clamp_min(1e-6)\n",
    "    x_scaled = (x - x_mean) / x_std\n",
    "\n",
    "    # -------- Project each scaled curve to basis coefficients\n",
    "    coef_all = project_to_basis_coeffs(x_scaled, B, W_vec, ridge=1e-6)  # [n_year, n_basis]\n",
    "\n",
    "    # -------- Train on years <= 2010, forecast 2011-2015\n",
    "    train_end_year = 2010\n",
    "    forecast_years = [2011, 2012, 2013, 2014, 2015]\n",
    "\n",
    "    year_to_idx = {y: i for i, y in enumerate(years)}\n",
    "    idx_train = [year_to_idx[y] for y in years if y <= train_end_year]\n",
    "    idx_fore_target = [year_to_idx[y] for y in forecast_years]\n",
    "\n",
    "    coef_train = coef_all[idx_train]\n",
    "    x_train = x_scaled[idx_train]  # train targets are curves (scaled)\n",
    "\n",
    "    # -------- Train FAE\n",
    "    device = \"cpu\"\n",
    "    model, history = train_fae(\n",
    "        coef_train=coef_train,\n",
    "        x_train=x_train,\n",
    "        B=B,\n",
    "        n_rep=7,\n",
    "        hidden=64,\n",
    "        nonlinear=True,\n",
    "        epochs=3000,\n",
    "        batch_size=16,\n",
    "        lr=2e-3,\n",
    "        lamb=1e-4,\n",
    "        device=device,\n",
    "        log_every=200\n",
    "    )\n",
    "\n",
    "    plot_training_loss(history)\n",
    "\n",
    "    # -------- Encode latent series in time order\n",
    "    h_train = encode_latents(model, coef_train, device=device)  # [T_train, K]\n",
    "\n",
    "    # -------- VAR forecast latents\n",
    "    h_fore, var_res = fit_var_forecast(h_train, steps=len(forecast_years), maxlags=2)\n",
    "\n",
    "    # -------- Decode forecast latents -> forecast scaled curves\n",
    "    x_fore_scaled = decode_latents_to_curves(model, h_fore, B, device=device)  # [5, n_age]\n",
    "\n",
    "    # -------- Unscale forecasts back to original scale\n",
    "    x_fore = x_fore_scaled * x_std.numpy() + x_mean.numpy()  # broadcasts\n",
    "\n",
    "    # -------- Plot real vs forecast (2010-2015)\n",
    "    plot_real_vs_forecast(\n",
    "        ages=ages,\n",
    "        years=years,\n",
    "        x_real=x_raw,\n",
    "        forecast_years=forecast_years,\n",
    "        x_fore=x_fore,\n",
    "        title=\"Australia fertility: Real vs Forecast (FAE latents + VAR)\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56988b5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
